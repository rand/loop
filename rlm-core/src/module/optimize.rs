//! DSPy-style optimization for modules.
//!
//! This module provides the `Optimizer` trait and `BootstrapFewShot` implementation
//! for automatic prompt optimization through few-shot demonstration selection.
//!
//! ## Overview
//!
//! The optimization process:
//! 1. Run the module on training examples with high temperature for diversity
//! 2. Evaluate each output against ground truth using a metric function
//! 3. Select the best-performing examples as demonstrations
//! 4. Inject selected demonstrations into the module's predictors
//!
//! ## Example
//!
//! ```ignore
//! use rlm_core::module::{BootstrapFewShot, Example, Module, Optimizer, Predict};
//!
//! // Create training data
//! let trainset = vec![
//!     Example::new(inputs1, outputs1),
//!     Example::new(inputs2, outputs2),
//! ];
//!
//! // Create optimizer
//! let optimizer = BootstrapFewShot::default();
//!
//! // Optimize the module
//! let optimized = optimizer.compile(
//!     module,
//!     &trainset,
//!     |pred, gold| metrics::exact_match(pred, gold),
//! ).await?;
//!
//! // Use the optimized module
//! let output = optimized.forward(inputs).await?;
//! ```

use std::collections::HashSet;
use std::marker::PhantomData;
use std::path::Path;
use std::sync::Arc;

use async_trait::async_trait;
use serde::{Deserialize, Serialize};

use super::{Demonstration, ErasedDemonstration, Example, Module, Predictor};
use crate::error::{Error, Result};
use crate::llm::LLMClient;
use crate::signature::Signature;

/// Object-safe metric trait for evaluating predictions against ground truth.
pub trait Metric<T>: Send + Sync {
    /// Score a predicted output against gold output (0.0..=1.0 expected).
    fn score(&self, predicted: &T, gold: &T) -> f64;

    /// Human-readable metric name for debugging/reporting.
    fn name(&self) -> &str;
}

impl<T, F> Metric<T> for F
where
    F: Fn(&T, &T) -> f64 + Send + Sync,
{
    fn score(&self, predicted: &T, gold: &T) -> f64 {
        self(predicted, gold)
    }

    fn name(&self) -> &str {
        std::any::type_name::<F>()
    }
}

/// Named wrapper for closure-backed metrics.
pub struct NamedMetric<T> {
    name: String,
    scorer: Arc<dyn Fn(&T, &T) -> f64 + Send + Sync>,
}

impl<T> NamedMetric<T> {
    /// Create a named metric from a closure.
    pub fn new(
        name: impl Into<String>,
        scorer: impl Fn(&T, &T) -> f64 + Send + Sync + 'static,
    ) -> Self {
        Self {
            name: name.into(),
            scorer: Arc::new(scorer),
        }
    }
}

impl<T> Metric<T> for NamedMetric<T> {
    fn score(&self, predicted: &T, gold: &T) -> f64 {
        (self.scorer)(predicted, gold)
    }

    fn name(&self) -> &str {
        &self.name
    }
}

/// Shared metric trait-object type used by optimizers.
pub type MetricFn<T> = Arc<dyn Metric<T>>;

/// Trait for optimizers that compile modules with demonstrations.
///
/// Optimizers take a module, training data, and a metric function, then
/// produce an optimized module with injected few-shot demonstrations.
#[async_trait]
pub trait Optimizer: Send + Sync {
    /// Compile a module with optimized demonstrations.
    ///
    /// # Arguments
    ///
    /// * `module` - The module to optimize
    /// * `trainset` - Training examples with inputs and expected outputs
    /// * `metric` - Function to evaluate predictions against ground truth (returns 0.0-1.0)
    ///
    /// # Returns
    ///
    /// An `OptimizedModule` wrapping the original with injected demonstrations.
    async fn compile<S, M>(
        &self,
        module: M,
        trainset: &[Example<S>],
        metric: MetricFn<S::Outputs>,
    ) -> Result<OptimizedModule<S, M>>
    where
        S: Signature + 'static,
        M: Module<Sig = S> + Clone + 'static;
}

/// Configuration for the BootstrapFewShot optimizer.
///
/// BootstrapFewShot runs the module on training data, evaluates outputs
/// using a metric, and selects the best examples as few-shot demonstrations.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BootstrapFewShot {
    /// Maximum number of bootstrapped demonstrations to include.
    /// These are generated by running the module on training data.
    pub max_bootstrapped_demos: usize,

    /// Maximum number of labeled demonstrations from trainset.
    /// These are examples directly from the training set.
    pub max_labeled_demos: usize,

    /// Number of bootstrap rounds.
    /// Each round generates new demonstrations from the current module state.
    pub max_rounds: usize,

    /// Minimum metric score for a demonstration to be included.
    /// Examples scoring below this threshold are filtered out.
    pub metric_threshold: f64,

    /// Temperature for LLM sampling during bootstrap.
    /// Higher temperature produces more diverse outputs.
    pub temperature: f64,

    /// Whether to include reasoning traces in demonstrations.
    pub include_reasoning: bool,

    /// Whether to deduplicate demonstrations by output.
    pub deduplicate: bool,
}

impl Default for BootstrapFewShot {
    fn default() -> Self {
        Self {
            max_bootstrapped_demos: 4,
            max_labeled_demos: 16,
            max_rounds: 1,
            metric_threshold: 0.0,
            temperature: 1.0,
            include_reasoning: true,
            deduplicate: true,
        }
    }
}

impl BootstrapFewShot {
    /// Create a new BootstrapFewShot optimizer with default settings.
    pub fn new() -> Self {
        Self::default()
    }

    /// Set maximum bootstrapped demonstrations.
    pub fn with_max_bootstrapped_demos(mut self, n: usize) -> Self {
        self.max_bootstrapped_demos = n;
        self
    }

    /// Set maximum labeled demonstrations.
    pub fn with_max_labeled_demos(mut self, n: usize) -> Self {
        self.max_labeled_demos = n;
        self
    }

    /// Set maximum bootstrap rounds.
    pub fn with_max_rounds(mut self, n: usize) -> Self {
        self.max_rounds = n;
        self
    }

    /// Set metric threshold.
    pub fn with_metric_threshold(mut self, threshold: f64) -> Self {
        self.metric_threshold = threshold;
        self
    }

    /// Set temperature for sampling.
    pub fn with_temperature(mut self, temp: f64) -> Self {
        self.temperature = temp;
        self
    }

    /// Disable reasoning traces in demonstrations.
    pub fn without_reasoning(mut self) -> Self {
        self.include_reasoning = false;
        self
    }

    /// Disable deduplication.
    pub fn without_deduplication(mut self) -> Self {
        self.deduplicate = false;
        self
    }

    /// Create a "greedy" configuration optimized for speed.
    pub fn greedy() -> Self {
        Self {
            max_bootstrapped_demos: 2,
            max_labeled_demos: 4,
            max_rounds: 1,
            metric_threshold: 0.5,
            temperature: 0.7,
            include_reasoning: false,
            deduplicate: true,
        }
    }

    /// Create a "thorough" configuration for best quality.
    pub fn thorough() -> Self {
        Self {
            max_bootstrapped_demos: 8,
            max_labeled_demos: 32,
            max_rounds: 3,
            metric_threshold: 0.0,
            temperature: 1.0,
            include_reasoning: true,
            deduplicate: true,
        }
    }
}

#[async_trait]
impl Optimizer for BootstrapFewShot {
    async fn compile<S, M>(
        &self,
        module: M,
        trainset: &[Example<S>],
        metric: MetricFn<S::Outputs>,
    ) -> Result<OptimizedModule<S, M>>
    where
        S: Signature + 'static,
        M: Module<Sig = S> + Clone + 'static,
    {
        if trainset.is_empty() {
            return Err(Error::Config("Training set is empty".to_string()));
        }

        let mut all_candidates: Vec<ScoredDemo<S>> = Vec::new();
        let mut stats = OptimizationStats::new(self.max_rounds);

        // Run bootstrap rounds
        for round in 0..self.max_rounds {
            stats.start_round(round);

            // Process each training example
            for example in trainset.iter() {
                // Run the module on the example inputs
                match module.forward(example.inputs.clone()).await {
                    Ok(predicted) => {
                        // Evaluate against ground truth
                        let score = metric.score(&predicted, &example.outputs);
                        stats.record_evaluation(score);

                        // Check if it meets threshold
                        if score >= self.metric_threshold {
                            let reasoning = if self.include_reasoning {
                                Some(build_bootstrap_reasoning_summary::<S>(
                                    &predicted,
                                    &example.outputs,
                                    score,
                                    round,
                                ))
                            } else {
                                None
                            };

                            let demo = ScoredDemo {
                                inputs: example.inputs.clone(),
                                outputs: predicted,
                                gold_outputs: example.outputs.clone(),
                                score,
                                reasoning,
                                round,
                            };
                            all_candidates.push(demo);
                            stats.record_candidate();
                        }
                    }
                    Err(e) => {
                        stats.record_error(e.to_string());
                    }
                }
            }

            stats.end_round();
        }

        // Also add labeled examples from trainset (with score 1.0)
        for example in trainset.iter().take(self.max_labeled_demos) {
            let demo = ScoredDemo {
                inputs: example.inputs.clone(),
                outputs: example.outputs.clone(),
                gold_outputs: example.outputs.clone(),
                score: 1.0,
                reasoning: if self.include_reasoning {
                    Some(build_labeled_reasoning_summary::<S>(&example.outputs))
                } else {
                    None
                },
                round: usize::MAX, // Sentinel for labeled demos
            };
            all_candidates.push(demo);
        }

        // Sort by score descending
        all_candidates.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));

        // Deduplicate if enabled
        let candidates = if self.deduplicate {
            deduplicate_demos(all_candidates)
        } else {
            all_candidates
        };

        // Select top demonstrations
        let total_demos = self.max_bootstrapped_demos + self.max_labeled_demos;
        let selected: Vec<Demonstration<S>> = candidates
            .into_iter()
            .take(total_demos)
            .map(|d| {
                let mut demo = Demonstration::new(d.inputs, d.outputs);
                if let Some(score) = Some(d.score) {
                    demo = demo.with_metric_score(score);
                }
                if let Some(reasoning) = d.reasoning {
                    demo = demo.set_reasoning(reasoning);
                }
                demo
            })
            .collect();

        stats.set_selected_count(selected.len());

        Ok(OptimizedModule {
            inner: module,
            demonstrations: selected,
            stats,
            _phantom: PhantomData,
        })
    }
}

/// Internal struct for tracking scored demonstrations during optimization.
struct ScoredDemo<S: Signature> {
    inputs: S::Inputs,
    outputs: S::Outputs,
    #[allow(dead_code)]
    gold_outputs: S::Outputs,
    score: f64,
    reasoning: Option<String>,
    #[allow(dead_code)]
    round: usize,
}

/// Deduplicate demonstrations by output (keeps highest scoring).
fn deduplicate_demos<S: Signature>(mut demos: Vec<ScoredDemo<S>>) -> Vec<ScoredDemo<S>> {
    // Already sorted by score descending, so first occurrence of each output wins
    let mut seen_outputs: HashSet<String> = HashSet::new();
    demos.retain(|d| {
        let output_str = serde_json::to_string(&d.outputs).unwrap_or_default();
        seen_outputs.insert(output_str)
    });
    demos
}

fn build_bootstrap_reasoning_summary<S: Signature>(
    predicted: &S::Outputs,
    gold: &S::Outputs,
    score: f64,
    round: usize,
) -> String {
    let predicted_json =
        serde_json::to_string(predicted).unwrap_or_else(|_| "<serialize_error>".to_string());
    let gold_json = serde_json::to_string(gold).unwrap_or_else(|_| "<serialize_error>".to_string());
    format!(
        "bootstrap_round={round}; metric_score={score:.4}; predicted={predicted_json}; gold={gold_json}"
    )
}

fn build_labeled_reasoning_summary<S: Signature>(gold: &S::Outputs) -> String {
    let gold_json = serde_json::to_string(gold).unwrap_or_else(|_| "<serialize_error>".to_string());
    format!("labeled_demo; metric_score=1.0000; gold={gold_json}")
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct PersistedOptimizationState {
    demonstrations: Vec<ErasedDemonstration>,
    stats: OptimizationStats,
}

/// A module wrapped with optimized demonstrations.
///
/// The optimized module injects the selected demonstrations into its
/// predictors before each forward pass.
pub struct OptimizedModule<S: Signature, M: Module<Sig = S>> {
    /// The underlying module.
    inner: M,
    /// Selected demonstrations to inject.
    demonstrations: Vec<Demonstration<S>>,
    /// Statistics about the optimization process.
    stats: OptimizationStats,
    _phantom: PhantomData<S>,
}

impl<S: Signature, M: Module<Sig = S>> OptimizedModule<S, M> {
    /// Get the optimization statistics.
    pub fn stats(&self) -> &OptimizationStats {
        &self.stats
    }

    /// Get the selected demonstrations.
    pub fn demonstrations(&self) -> &[Demonstration<S>] {
        &self.demonstrations
    }

    /// Get a reference to the inner module.
    pub fn inner(&self) -> &M {
        &self.inner
    }

    /// Get a mutable reference to the inner module.
    pub fn inner_mut(&mut self) -> &mut M {
        &mut self.inner
    }

    /// Unwrap and return the inner module.
    pub fn into_inner(self) -> M {
        self.inner
    }

    /// Save optimized demonstrations and stats to disk.
    pub fn save(&self, path: impl AsRef<Path>) -> Result<()> {
        let state = PersistedOptimizationState {
            demonstrations: self
                .demonstrations
                .iter()
                .map(ErasedDemonstration::from_typed)
                .collect(),
            stats: self.stats.clone(),
        };

        let serialized = serde_json::to_string_pretty(&state)?;
        std::fs::write(path.as_ref(), serialized).map_err(|e| {
            Error::Internal(format!("Failed to write optimized module state: {}", e))
        })?;
        Ok(())
    }

    /// Load optimized demonstrations and stats from disk for a module.
    pub fn load(module: M, path: impl AsRef<Path>) -> Result<Self> {
        let serialized = std::fs::read_to_string(path.as_ref()).map_err(|e| {
            Error::Internal(format!("Failed to read optimized module state: {}", e))
        })?;
        let state: PersistedOptimizationState = serde_json::from_str(&serialized)?;
        let demonstrations = state
            .demonstrations
            .into_iter()
            .map(|demo| {
                let inputs: S::Inputs = serde_json::from_value(demo.inputs)?;
                let outputs: S::Outputs = serde_json::from_value(demo.outputs)?;

                let mut typed = Demonstration::new(inputs, outputs);
                if let Some(reasoning) = demo.reasoning {
                    typed = typed.set_reasoning(reasoning);
                }
                if let Some(score) = demo.metric_score {
                    typed = typed.with_metric_score(score);
                }
                Ok(typed)
            })
            .collect::<Result<Vec<_>>>()?;

        Ok(Self {
            inner: module,
            demonstrations,
            stats: state.stats,
            _phantom: PhantomData,
        })
    }
}

#[async_trait]
impl<S: Signature + 'static, M: Module<Sig = S> + 'static> Module for OptimizedModule<S, M> {
    type Sig = S;

    async fn forward(&self, inputs: S::Inputs) -> Result<S::Outputs> {
        // Forward to inner module (demonstrations are already injected during compile)
        self.inner.forward(inputs).await
    }

    fn predictors(&self) -> Vec<&dyn Predictor> {
        self.inner.predictors()
    }

    fn set_lm(&mut self, lm: Arc<dyn LLMClient>) {
        self.inner.set_lm(lm);
    }

    fn get_lm(&self) -> Option<Arc<dyn LLMClient>> {
        self.inner.get_lm()
    }

    fn name(&self) -> &str {
        "OptimizedModule"
    }
}

/// Statistics about the optimization process.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationStats {
    /// Number of rounds executed.
    pub rounds_completed: usize,
    /// Total rounds planned.
    pub rounds_total: usize,
    /// Total examples evaluated.
    pub examples_evaluated: usize,
    /// Candidates that passed threshold.
    pub candidates_generated: usize,
    /// Final demonstrations selected.
    pub demonstrations_selected: usize,
    /// Mean metric score across all evaluations.
    pub mean_score: f64,
    /// Maximum metric score achieved.
    pub max_score: f64,
    /// Minimum metric score achieved.
    pub min_score: f64,
    /// Errors encountered during optimization.
    pub errors: Vec<String>,
    /// Per-round statistics.
    pub round_stats: Vec<RoundStats>,
}

impl OptimizationStats {
    /// Create new statistics.
    fn new(total_rounds: usize) -> Self {
        Self {
            rounds_completed: 0,
            rounds_total: total_rounds,
            examples_evaluated: 0,
            candidates_generated: 0,
            demonstrations_selected: 0,
            mean_score: 0.0,
            max_score: f64::NEG_INFINITY,
            min_score: f64::INFINITY,
            errors: Vec::new(),
            round_stats: Vec::new(),
        }
    }

    fn start_round(&mut self, _round: usize) {
        self.round_stats.push(RoundStats::default());
    }

    fn record_evaluation(&mut self, score: f64) {
        self.examples_evaluated += 1;
        if let Some(round) = self.round_stats.last_mut() {
            round.examples_evaluated += 1;
            round.total_score += score;
        }
        if score > self.max_score {
            self.max_score = score;
        }
        if score < self.min_score {
            self.min_score = score;
        }
    }

    fn record_candidate(&mut self) {
        self.candidates_generated += 1;
        if let Some(round) = self.round_stats.last_mut() {
            round.candidates_generated += 1;
        }
    }

    fn record_error(&mut self, error: String) {
        self.errors.push(error);
    }

    fn end_round(&mut self) {
        self.rounds_completed += 1;
        if let Some(round) = self.round_stats.last_mut() {
            if round.examples_evaluated > 0 {
                round.mean_score = round.total_score / round.examples_evaluated as f64;
            }
        }
    }

    fn set_selected_count(&mut self, count: usize) {
        self.demonstrations_selected = count;
        // Calculate overall mean score
        if self.examples_evaluated > 0 {
            let total: f64 = self.round_stats.iter().map(|r| r.total_score).sum();
            self.mean_score = total / self.examples_evaluated as f64;
        }
    }

    /// Generate a summary string.
    pub fn summary(&self) -> String {
        format!(
            "Optimization: {} rounds, {} examples, {} candidates, {} selected (mean={:.3}, max={:.3})",
            self.rounds_completed,
            self.examples_evaluated,
            self.candidates_generated,
            self.demonstrations_selected,
            self.mean_score,
            if self.max_score > f64::NEG_INFINITY { self.max_score } else { 0.0 }
        )
    }
}

/// Statistics for a single optimization round.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct RoundStats {
    /// Examples evaluated this round.
    pub examples_evaluated: usize,
    /// Candidates generated this round.
    pub candidates_generated: usize,
    /// Mean score this round.
    pub mean_score: f64,
    /// Total score (for computing mean).
    #[serde(skip)]
    total_score: f64,
}

/// Common metric functions for evaluation.
pub mod metrics {
    use std::collections::HashSet;

    /// Exact match metric - returns 1.0 if equal, 0.0 otherwise.
    ///
    /// Useful for classification tasks where the output must exactly match.
    pub fn exact_match<T: PartialEq>(predicted: &T, gold: &T) -> f64 {
        if predicted == gold {
            1.0
        } else {
            0.0
        }
    }

    /// F1 score for set-based outputs (e.g., extraction tasks).
    ///
    /// Computes the harmonic mean of precision and recall.
    pub fn f1_score(predicted: &[String], gold: &[String]) -> f64 {
        if predicted.is_empty() && gold.is_empty() {
            return 1.0;
        }
        if predicted.is_empty() || gold.is_empty() {
            return 0.0;
        }

        let pred_set: HashSet<_> = predicted.iter().collect();
        let gold_set: HashSet<_> = gold.iter().collect();

        let true_positives = pred_set.intersection(&gold_set).count() as f64;
        let precision = true_positives / predicted.len() as f64;
        let recall = true_positives / gold.len() as f64;

        if precision + recall == 0.0 {
            0.0
        } else {
            2.0 * (precision * recall) / (precision + recall)
        }
    }

    /// Jaccard similarity for set-based outputs.
    ///
    /// Computes |intersection| / |union|.
    pub fn jaccard_similarity(predicted: &[String], gold: &[String]) -> f64 {
        if predicted.is_empty() && gold.is_empty() {
            return 1.0;
        }

        let pred_set: HashSet<_> = predicted.iter().collect();
        let gold_set: HashSet<_> = gold.iter().collect();

        let intersection = pred_set.intersection(&gold_set).count() as f64;
        let union = pred_set.union(&gold_set).count() as f64;

        if union == 0.0 {
            0.0
        } else {
            intersection / union
        }
    }

    /// Substring containment metric.
    ///
    /// Returns 1.0 if gold is contained in predicted, 0.0 otherwise.
    pub fn contains(predicted: &str, gold: &str) -> f64 {
        if predicted.contains(gold) {
            1.0
        } else {
            0.0
        }
    }

    /// Case-insensitive exact match.
    pub fn exact_match_ignore_case(predicted: &str, gold: &str) -> f64 {
        if predicted.to_lowercase() == gold.to_lowercase() {
            1.0
        } else {
            0.0
        }
    }

    /// Normalized edit distance similarity.
    ///
    /// Returns 1.0 - (edit_distance / max_length), where 1.0 is identical.
    pub fn edit_distance_similarity(predicted: &str, gold: &str) -> f64 {
        if predicted == gold {
            return 1.0;
        }
        if predicted.is_empty() || gold.is_empty() {
            return 0.0;
        }

        let distance = levenshtein_distance(predicted, gold);
        let max_len = predicted.len().max(gold.len());

        1.0 - (distance as f64 / max_len as f64)
    }

    /// Compute Levenshtein edit distance.
    fn levenshtein_distance(a: &str, b: &str) -> usize {
        let a_chars: Vec<char> = a.chars().collect();
        let b_chars: Vec<char> = b.chars().collect();
        let m = a_chars.len();
        let n = b_chars.len();

        if m == 0 {
            return n;
        }
        if n == 0 {
            return m;
        }

        let mut dp = vec![vec![0usize; n + 1]; m + 1];

        for i in 0..=m {
            dp[i][0] = i;
        }
        for j in 0..=n {
            dp[0][j] = j;
        }

        for i in 1..=m {
            for j in 1..=n {
                let cost = if a_chars[i - 1] == b_chars[j - 1] { 0 } else { 1 };
                dp[i][j] = (dp[i - 1][j] + 1)
                    .min(dp[i][j - 1] + 1)
                    .min(dp[i - 1][j - 1] + cost);
            }
        }

        dp[m][n]
    }

    /// Combine multiple metrics with weights.
    ///
    /// # Example
    ///
    /// ```ignore
    /// let combined = combine_metrics(
    ///     &[
    ///         (Box::new(|p, g| exact_match(p, g)), 0.7),
    ///         (Box::new(|p, g| contains(p, g)), 0.3),
    ///     ],
    ///     &predicted,
    ///     &gold,
    /// );
    /// ```
    pub fn combine_weighted<T, F>(metrics: &[(F, f64)], predicted: &T, gold: &T) -> f64
    where
        F: Fn(&T, &T) -> f64,
    {
        let total_weight: f64 = metrics.iter().map(|(_, w)| w).sum();
        if total_weight == 0.0 {
            return 0.0;
        }

        let weighted_sum: f64 = metrics
            .iter()
            .map(|(metric, weight)| metric(predicted, gold) * weight)
            .sum();

        weighted_sum / total_weight
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::signature::{FieldSpec, Signature};
    use async_trait::async_trait;
    use std::sync::Arc;
    use tempfile::NamedTempFile;

    #[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
    struct MockInputs {
        text: String,
    }

    #[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
    struct MockOutputs {
        result: String,
    }

    struct MockSignature;

    impl Signature for MockSignature {
        type Inputs = MockInputs;
        type Outputs = MockOutputs;

        fn instructions() -> &'static str {
            "Echo input text"
        }

        fn input_fields() -> Vec<FieldSpec> {
            vec![]
        }

        fn output_fields() -> Vec<FieldSpec> {
            vec![]
        }
    }

    #[derive(Clone, Default)]
    struct MockModule;

    #[async_trait]
    impl Module for MockModule {
        type Sig = MockSignature;

        async fn forward(&self, inputs: MockInputs) -> Result<MockOutputs> {
            Ok(MockOutputs {
                result: inputs.text,
            })
        }

        fn predictors(&self) -> Vec<&dyn Predictor> {
            Vec::new()
        }

        fn set_lm(&mut self, _lm: Arc<dyn LLMClient>) {}

        fn get_lm(&self) -> Option<Arc<dyn LLMClient>> {
            None
        }
    }

    fn mock_trainset() -> Vec<Example<MockSignature>> {
        vec![
            Example::new(
                MockInputs {
                    text: "alpha".to_string(),
                },
                MockOutputs {
                    result: "alpha".to_string(),
                },
            ),
            Example::new(
                MockInputs {
                    text: "beta".to_string(),
                },
                MockOutputs {
                    result: "beta".to_string(),
                },
            ),
        ]
    }

    #[test]
    fn test_bootstrap_config_default() {
        let config = BootstrapFewShot::default();
        assert_eq!(config.max_bootstrapped_demos, 4);
        assert_eq!(config.max_labeled_demos, 16);
        assert_eq!(config.max_rounds, 1);
        assert_eq!(config.metric_threshold, 0.0);
        assert_eq!(config.temperature, 1.0);
    }

    #[test]
    fn test_bootstrap_config_builder() {
        let config = BootstrapFewShot::new()
            .with_max_bootstrapped_demos(8)
            .with_max_labeled_demos(32)
            .with_max_rounds(3)
            .with_metric_threshold(0.5)
            .with_temperature(0.8)
            .without_reasoning();

        assert_eq!(config.max_bootstrapped_demos, 8);
        assert_eq!(config.max_labeled_demos, 32);
        assert_eq!(config.max_rounds, 3);
        assert_eq!(config.metric_threshold, 0.5);
        assert_eq!(config.temperature, 0.8);
        assert!(!config.include_reasoning);
    }

    #[test]
    fn test_bootstrap_presets() {
        let greedy = BootstrapFewShot::greedy();
        assert_eq!(greedy.max_bootstrapped_demos, 2);
        assert_eq!(greedy.max_rounds, 1);

        let thorough = BootstrapFewShot::thorough();
        assert_eq!(thorough.max_bootstrapped_demos, 8);
        assert_eq!(thorough.max_rounds, 3);
    }

    #[test]
    fn test_metrics_exact_match() {
        assert_eq!(metrics::exact_match(&"hello", &"hello"), 1.0);
        assert_eq!(metrics::exact_match(&"hello", &"world"), 0.0);
        assert_eq!(metrics::exact_match(&42, &42), 1.0);
        assert_eq!(metrics::exact_match(&42, &43), 0.0);
    }

    #[test]
    fn test_metrics_f1_score() {
        // Perfect match
        let pred = vec!["a".to_string(), "b".to_string()];
        let gold = vec!["a".to_string(), "b".to_string()];
        assert!((metrics::f1_score(&pred, &gold) - 1.0).abs() < 0.001);

        // Partial match
        let pred = vec!["a".to_string(), "c".to_string()];
        let gold = vec!["a".to_string(), "b".to_string()];
        let f1 = metrics::f1_score(&pred, &gold);
        assert!((f1 - 0.5).abs() < 0.001); // 1 TP, 1 FP, 1 FN -> F1 = 0.5

        // No match
        let pred = vec!["x".to_string()];
        let gold = vec!["y".to_string()];
        assert_eq!(metrics::f1_score(&pred, &gold), 0.0);

        // Empty cases
        assert_eq!(metrics::f1_score(&vec![], &vec![]), 1.0);
        assert_eq!(metrics::f1_score(&vec!["a".to_string()], &vec![]), 0.0);
    }

    #[test]
    fn test_metrics_jaccard() {
        let pred = vec!["a".to_string(), "b".to_string(), "c".to_string()];
        let gold = vec!["b".to_string(), "c".to_string(), "d".to_string()];

        // Intersection: {b, c} = 2, Union: {a, b, c, d} = 4
        let jaccard = metrics::jaccard_similarity(&pred, &gold);
        assert!((jaccard - 0.5).abs() < 0.001);
    }

    #[test]
    fn test_metrics_edit_distance() {
        assert_eq!(metrics::edit_distance_similarity("hello", "hello"), 1.0);
        assert!(metrics::edit_distance_similarity("hello", "helo") >= 0.8);
        assert!(metrics::edit_distance_similarity("abc", "xyz") < 0.5);
    }

    #[test]
    fn test_metrics_contains() {
        assert_eq!(metrics::contains("hello world", "world"), 1.0);
        assert_eq!(metrics::contains("hello", "world"), 0.0);
    }

    #[test]
    fn test_metrics_combine_weighted() {
        let metrics_list: Vec<(fn(&i32, &i32) -> f64, f64)> = vec![
            (|a: &i32, b: &i32| if a == b { 1.0 } else { 0.0 }, 0.5),
            (|a: &i32, b: &i32| if (a - b).abs() <= 1 { 1.0 } else { 0.0 }, 0.5),
        ];

        // Exact match: both metrics return 1.0
        let score = metrics::combine_weighted(&metrics_list, &5, &5);
        assert!((score - 1.0).abs() < 0.001);

        // Close match: only second metric returns 1.0
        let score = metrics::combine_weighted(&metrics_list, &5, &6);
        assert!((score - 0.5).abs() < 0.001);

        // No match: both return 0.0
        let score = metrics::combine_weighted(&metrics_list, &5, &10);
        assert!((score - 0.0).abs() < 0.001);
    }

    #[test]
    fn test_metric_trait_object_with_function_pointer() {
        let metric: MetricFn<MockOutputs> = Arc::new(metrics::exact_match);
        let predicted = MockOutputs {
            result: "ok".to_string(),
        };
        let gold = MockOutputs {
            result: "ok".to_string(),
        };

        assert_eq!(metric.score(&predicted, &gold), 1.0);
        assert!(!metric.name().is_empty());
    }

    #[test]
    fn test_named_metric_trait_object() {
        let metric: MetricFn<MockOutputs> = Arc::new(NamedMetric::new(
            "named_exact_match",
            |predicted: &MockOutputs, gold: &MockOutputs| metrics::exact_match(predicted, gold),
        ));

        let predicted = MockOutputs {
            result: "left".to_string(),
        };
        let gold = MockOutputs {
            result: "right".to_string(),
        };

        assert_eq!(metric.name(), "named_exact_match");
        assert_eq!(metric.score(&predicted, &gold), 0.0);
    }

    #[test]
    fn test_optimization_stats() {
        let mut stats = OptimizationStats::new(2);

        stats.start_round(0);
        stats.record_evaluation(0.8);
        stats.record_evaluation(0.6);
        stats.record_candidate();
        stats.end_round();

        stats.start_round(1);
        stats.record_evaluation(0.9);
        stats.record_candidate();
        stats.record_candidate();
        stats.end_round();

        stats.set_selected_count(3);

        assert_eq!(stats.rounds_completed, 2);
        assert_eq!(stats.examples_evaluated, 3);
        assert_eq!(stats.candidates_generated, 3);
        assert_eq!(stats.demonstrations_selected, 3);
        assert!((stats.max_score - 0.9).abs() < 0.001);
        assert!((stats.min_score - 0.6).abs() < 0.001);
    }

    #[tokio::test]
    async fn test_compile_captures_reasoning_when_enabled() {
        let optimizer = BootstrapFewShot::new()
            .with_max_bootstrapped_demos(2)
            .with_max_labeled_demos(0)
            .with_max_rounds(1);
        let module = MockModule;
        let trainset = mock_trainset();
        let metric: MetricFn<MockOutputs> = Arc::new(metrics::exact_match);

        let optimized = optimizer
            .compile(module, &trainset, metric)
            .await
            .expect("compile should succeed");
        assert!(!optimized.demonstrations().is_empty());
        assert!(optimized
            .demonstrations()
            .iter()
            .all(|d| d.reasoning.is_some()));
    }

    #[tokio::test]
    async fn test_compile_skips_reasoning_when_disabled() {
        let optimizer = BootstrapFewShot::new()
            .with_max_bootstrapped_demos(2)
            .with_max_labeled_demos(0)
            .with_max_rounds(1)
            .without_reasoning();
        let module = MockModule;
        let trainset = mock_trainset();
        let metric: MetricFn<MockOutputs> = Arc::new(metrics::exact_match);

        let optimized = optimizer
            .compile(module, &trainset, metric)
            .await
            .expect("compile should succeed");
        assert!(optimized
            .demonstrations()
            .iter()
            .all(|d| d.reasoning.is_none()));
    }

    #[tokio::test]
    async fn test_optimized_module_save_and_load_roundtrip() {
        let optimizer = BootstrapFewShot::new()
            .with_max_bootstrapped_demos(2)
            .with_max_labeled_demos(0)
            .with_max_rounds(1);
        let module = MockModule;
        let trainset = mock_trainset();
        let metric: MetricFn<MockOutputs> = Arc::new(metrics::exact_match);

        let optimized = optimizer
            .compile(module.clone(), &trainset, metric)
            .await
            .expect("compile should succeed");

        let temp = NamedTempFile::new().expect("temp file should be created");
        optimized
            .save(temp.path())
            .expect("optimized state should be saved");

        let loaded = OptimizedModule::<MockSignature, MockModule>::load(module, temp.path())
            .expect("optimized state should be loaded");

        assert_eq!(
            loaded.demonstrations().len(),
            optimized.demonstrations().len()
        );
        assert_eq!(
            loaded.stats().demonstrations_selected,
            optimized.stats().demonstrations_selected
        );
    }
}
